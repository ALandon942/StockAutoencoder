{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c18b9d-ed70-4735-8d22-4951f5d95c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e4c36-f62a-4e12-8eb5-a101898d385a",
   "metadata": {},
   "source": [
    "# Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d0ed0-e914-4825-a425-9fded582daf5",
   "metadata": {},
   "source": [
    "To see how the sequential autoencoder approach (TODO describe) performs in finding factors that are relevant to asset-price anomalies, compared to the economics-informed PCA approach. Performance of each model is measured by how much of the variance is explained by the factors found. If the autoencoder model can match the performance of the economics-informed PCA one, that suggests that it can learn an equally informative set of economic knowledge from the trends in the data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bf95b-2d3b-4d05-a6d8-81bfdc8b0995",
   "metadata": {},
   "source": [
    "### Load and organize returns data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d262c4-10a6-481d-969f-4f12f57da7e7",
   "metadata": {},
   "source": [
    "The data set (see citations) is set up as follows:\n",
    "* One group of CSV files per anomaly, identified by filename prefix. We are only interested in the group prefixed `ret` for anomalous returns.\n",
    "* In this group are 55 files, one per characteristic of an individual stock, such as price. Characteristic is indicated by filename suffix.\n",
    "* Example file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f909bf14-47d0-456f-9103-17d8abcd2bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>07/1963</th>\n",
       "      <td>-0.016110</td>\n",
       "      <td>-0.016420</td>\n",
       "      <td>-0.014429</td>\n",
       "      <td>-0.012837</td>\n",
       "      <td>-0.011873</td>\n",
       "      <td>-0.006876</td>\n",
       "      <td>-0.005049</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>-0.000842</td>\n",
       "      <td>0.003301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08/1963</th>\n",
       "      <td>0.032904</td>\n",
       "      <td>0.027416</td>\n",
       "      <td>0.048415</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.071222</td>\n",
       "      <td>0.060035</td>\n",
       "      <td>0.053034</td>\n",
       "      <td>0.064582</td>\n",
       "      <td>0.060898</td>\n",
       "      <td>0.048308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09/1963</th>\n",
       "      <td>-0.014653</td>\n",
       "      <td>-0.014772</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>-0.024909</td>\n",
       "      <td>-0.014166</td>\n",
       "      <td>-0.006128</td>\n",
       "      <td>-0.022373</td>\n",
       "      <td>-0.033295</td>\n",
       "      <td>-0.019617</td>\n",
       "      <td>-0.006241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/1963</th>\n",
       "      <td>0.014183</td>\n",
       "      <td>0.015139</td>\n",
       "      <td>0.020362</td>\n",
       "      <td>-0.001361</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>-0.009800</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.049630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/1963</th>\n",
       "      <td>-0.034110</td>\n",
       "      <td>-0.011294</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.003260</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>-0.007177</td>\n",
       "      <td>-0.007279</td>\n",
       "      <td>-0.014306</td>\n",
       "      <td>-0.010153</td>\n",
       "      <td>-0.003912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>08/2019</th>\n",
       "      <td>-0.078559</td>\n",
       "      <td>-0.102274</td>\n",
       "      <td>-0.074431</td>\n",
       "      <td>-0.060736</td>\n",
       "      <td>-0.054562</td>\n",
       "      <td>-0.039603</td>\n",
       "      <td>-0.042928</td>\n",
       "      <td>-0.023711</td>\n",
       "      <td>-0.003197</td>\n",
       "      <td>-0.014348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09/2019</th>\n",
       "      <td>0.041238</td>\n",
       "      <td>0.034295</td>\n",
       "      <td>0.047052</td>\n",
       "      <td>0.030790</td>\n",
       "      <td>0.034646</td>\n",
       "      <td>0.025590</td>\n",
       "      <td>0.043141</td>\n",
       "      <td>0.031079</td>\n",
       "      <td>0.013854</td>\n",
       "      <td>-0.000509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/2019</th>\n",
       "      <td>-0.022744</td>\n",
       "      <td>0.022176</td>\n",
       "      <td>0.015909</td>\n",
       "      <td>-0.007088</td>\n",
       "      <td>0.042427</td>\n",
       "      <td>0.020122</td>\n",
       "      <td>0.013390</td>\n",
       "      <td>0.009148</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>0.033841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11/2019</th>\n",
       "      <td>0.024839</td>\n",
       "      <td>0.057429</td>\n",
       "      <td>0.065173</td>\n",
       "      <td>0.046381</td>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.030197</td>\n",
       "      <td>0.028627</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>0.035744</td>\n",
       "      <td>0.045679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12/2019</th>\n",
       "      <td>0.129850</td>\n",
       "      <td>0.038591</td>\n",
       "      <td>0.045261</td>\n",
       "      <td>0.048887</td>\n",
       "      <td>0.033986</td>\n",
       "      <td>0.041024</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.029102</td>\n",
       "      <td>0.027704</td>\n",
       "      <td>0.026909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>678 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               p1        p2        p3        p4        p5        p6        p7  \\\n",
       "date                                                                            \n",
       "07/1963 -0.016110 -0.016420 -0.014429 -0.012837 -0.011873 -0.006876 -0.005049   \n",
       "08/1963  0.032904  0.027416  0.048415  0.048101  0.071222  0.060035  0.053034   \n",
       "09/1963 -0.014653 -0.014772 -0.004352 -0.024909 -0.014166 -0.006128 -0.022373   \n",
       "10/1963  0.014183  0.015139  0.020362 -0.001361  0.012925 -0.009800  0.015713   \n",
       "11/1963 -0.034110 -0.011294  0.016071  0.003260  0.002138 -0.007177 -0.007279   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "08/2019 -0.078559 -0.102274 -0.074431 -0.060736 -0.054562 -0.039603 -0.042928   \n",
       "09/2019  0.041238  0.034295  0.047052  0.030790  0.034646  0.025590  0.043141   \n",
       "10/2019 -0.022744  0.022176  0.015909 -0.007088  0.042427  0.020122  0.013390   \n",
       "11/2019  0.024839  0.057429  0.065173  0.046381  0.038076  0.030197  0.028627   \n",
       "12/2019  0.129850  0.038591  0.045261  0.048887  0.033986  0.041024  0.019487   \n",
       "\n",
       "               p8        p9       p10  \n",
       "date                                   \n",
       "07/1963  0.000657 -0.000842  0.003301  \n",
       "08/1963  0.064582  0.060898  0.048308  \n",
       "09/1963 -0.033295 -0.019617 -0.006241  \n",
       "10/1963  0.001680  0.013357  0.049630  \n",
       "11/1963 -0.014306 -0.010153 -0.003912  \n",
       "...           ...       ...       ...  \n",
       "08/2019 -0.023711 -0.003197 -0.014348  \n",
       "09/2019  0.031079  0.013854 -0.000509  \n",
       "10/2019  0.009148  0.010597  0.033841  \n",
       "11/2019  0.037456  0.035744  0.045679  \n",
       "12/2019  0.029102  0.027704  0.026909  \n",
       "\n",
       "[678 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DataSet/monthly/ret10_price.csv', index_col='date')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975e5ed5-99b2-413f-8222-5995c466c355",
   "metadata": {},
   "source": [
    "* For each characteristic, all the stocks under consideration are sorted into deciles by that characteristic. Each decile is treated as a portfolio labeled p1 through p10 and these are the columns of the corresponding file.\n",
    "    * One complication is that some characteristics are categorical or otherwise don't have meaningful deciles. These files still have columns p1-p10 but some columns are disused and empty.\n",
    "* Rows correspond to dates. There is one row per month in the range ~1963-2019.\n",
    "    * A cell in a row may be unpopulated due to missing data for that month even when the column as a whole isn't empty.\n",
    "    * Different files cover slightly different date ranges.\n",
    "* Individual cells are the value-weighted returns for each portfolio during each month. For example, the bottom right cell above shows that in Dec. 2019, a portfolio of all stocks priced in the 10th decile had a return of 0.026909."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b651d4-abfb-4ddf-80ff-f9c4d6927c16",
   "metadata": {},
   "source": [
    "#### Start by merging all characteristics together into one data frame\n",
    "Merge will take place horizontally so that the result will still have one row per month and one column per portfolio, but will include a set of decile-sorted portfolios for every characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc069082-50d4-4b7e-a4f9-cbe9d87bbe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55 files of anomalous-return data\n",
      "Summary:\n",
      "\t5 non-decile characteristics: {'exchsw', 'fscore', 'repurch', 'ipo', 'debtiss'}\n",
      "\t9 characteristics with missing cells: {'roe', 'gltnoa', 'sue', 'rome', 'roa', 'nissm', 'nissa', 'invaci', 'age'}\n",
      "\t41 characteristics with complete data: {'valmomprof', 'prof', 'accruals', 'valmom', 'value', 'sp', 'lev', 'sgrowth', 'inv', 'roea', 'dur', 'growth', 'valuem', 'indrrev', 'season', 'noa', 'divp', 'cfp', 'gmargins', 'invcap', 'indmom', 'igrowth', 'divg', 'indmomrev', 'price', 'ivol', 'ciss', 'mom12', 'valprof', 'indrrevlv', 'shortint', 'aturnover', 'lrrev', 'strev', 'mom', 'shvol', 'betaarb', 'roaa', 'size', 'momrev', 'ep'}\n",
      "Using only characteristics with complete data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accruals_p1</th>\n",
       "      <th>accruals_p2</th>\n",
       "      <th>accruals_p3</th>\n",
       "      <th>accruals_p4</th>\n",
       "      <th>accruals_p5</th>\n",
       "      <th>accruals_p6</th>\n",
       "      <th>accruals_p7</th>\n",
       "      <th>accruals_p8</th>\n",
       "      <th>accruals_p9</th>\n",
       "      <th>accruals_p10</th>\n",
       "      <th>...</th>\n",
       "      <th>sp_p1</th>\n",
       "      <th>sp_p2</th>\n",
       "      <th>sp_p3</th>\n",
       "      <th>sp_p4</th>\n",
       "      <th>sp_p5</th>\n",
       "      <th>sp_p6</th>\n",
       "      <th>sp_p7</th>\n",
       "      <th>sp_p8</th>\n",
       "      <th>sp_p9</th>\n",
       "      <th>sp_p10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963-07-01</th>\n",
       "      <td>-0.012826</td>\n",
       "      <td>-0.035516</td>\n",
       "      <td>-0.001749</td>\n",
       "      <td>-0.002739</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.003346</td>\n",
       "      <td>-0.005244</td>\n",
       "      <td>-0.005873</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.016596</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.002039</td>\n",
       "      <td>-0.021691</td>\n",
       "      <td>-0.013344</td>\n",
       "      <td>-0.018734</td>\n",
       "      <td>-0.020539</td>\n",
       "      <td>-0.006043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-08-01</th>\n",
       "      <td>0.067188</td>\n",
       "      <td>0.075105</td>\n",
       "      <td>0.065833</td>\n",
       "      <td>0.059440</td>\n",
       "      <td>0.050219</td>\n",
       "      <td>0.043807</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>0.097553</td>\n",
       "      <td>0.046265</td>\n",
       "      <td>0.069205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045485</td>\n",
       "      <td>0.041143</td>\n",
       "      <td>0.048661</td>\n",
       "      <td>0.059708</td>\n",
       "      <td>0.069453</td>\n",
       "      <td>0.071549</td>\n",
       "      <td>0.057740</td>\n",
       "      <td>0.047063</td>\n",
       "      <td>0.083413</td>\n",
       "      <td>0.043130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-09-01</th>\n",
       "      <td>0.027254</td>\n",
       "      <td>-0.036061</td>\n",
       "      <td>-0.025227</td>\n",
       "      <td>-0.021122</td>\n",
       "      <td>-0.031047</td>\n",
       "      <td>-0.020266</td>\n",
       "      <td>0.006704</td>\n",
       "      <td>-0.001180</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>-0.001218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002031</td>\n",
       "      <td>-0.029466</td>\n",
       "      <td>-0.023707</td>\n",
       "      <td>-0.004787</td>\n",
       "      <td>-0.016817</td>\n",
       "      <td>-0.015236</td>\n",
       "      <td>-0.005143</td>\n",
       "      <td>-0.021049</td>\n",
       "      <td>-0.009069</td>\n",
       "      <td>-0.033984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-10-01</th>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.051991</td>\n",
       "      <td>0.028729</td>\n",
       "      <td>0.022873</td>\n",
       "      <td>-0.002550</td>\n",
       "      <td>0.033786</td>\n",
       "      <td>0.020920</td>\n",
       "      <td>0.068880</td>\n",
       "      <td>0.138922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033032</td>\n",
       "      <td>-0.009433</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.060448</td>\n",
       "      <td>0.021997</td>\n",
       "      <td>0.033764</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.017601</td>\n",
       "      <td>0.038851</td>\n",
       "      <td>0.009602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-11-01</th>\n",
       "      <td>-0.007417</td>\n",
       "      <td>0.006792</td>\n",
       "      <td>-0.006046</td>\n",
       "      <td>-0.013141</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>-0.012780</td>\n",
       "      <td>0.020039</td>\n",
       "      <td>-0.004045</td>\n",
       "      <td>-0.008319</td>\n",
       "      <td>-0.065160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018028</td>\n",
       "      <td>-0.012895</td>\n",
       "      <td>-0.011324</td>\n",
       "      <td>-0.029286</td>\n",
       "      <td>-0.011138</td>\n",
       "      <td>-0.007688</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>-0.004890</td>\n",
       "      <td>-0.003218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>-0.011983</td>\n",
       "      <td>-0.026596</td>\n",
       "      <td>-0.033196</td>\n",
       "      <td>-0.019816</td>\n",
       "      <td>-0.017636</td>\n",
       "      <td>-0.034318</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>-0.013897</td>\n",
       "      <td>-0.017711</td>\n",
       "      <td>-0.003340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007943</td>\n",
       "      <td>-0.030568</td>\n",
       "      <td>-0.024177</td>\n",
       "      <td>-0.003217</td>\n",
       "      <td>-0.014945</td>\n",
       "      <td>-0.048403</td>\n",
       "      <td>-0.068869</td>\n",
       "      <td>-0.035887</td>\n",
       "      <td>-0.049141</td>\n",
       "      <td>-0.063963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>0.011086</td>\n",
       "      <td>0.027976</td>\n",
       "      <td>0.009426</td>\n",
       "      <td>0.025268</td>\n",
       "      <td>0.021759</td>\n",
       "      <td>0.024613</td>\n",
       "      <td>0.017311</td>\n",
       "      <td>0.012674</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-0.010182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019202</td>\n",
       "      <td>0.022650</td>\n",
       "      <td>0.037942</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>0.036637</td>\n",
       "      <td>0.016883</td>\n",
       "      <td>0.035709</td>\n",
       "      <td>0.039095</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>0.060977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>0.041932</td>\n",
       "      <td>0.034752</td>\n",
       "      <td>0.040694</td>\n",
       "      <td>-0.004829</td>\n",
       "      <td>0.014374</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.017040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028577</td>\n",
       "      <td>0.031458</td>\n",
       "      <td>0.015161</td>\n",
       "      <td>0.005760</td>\n",
       "      <td>0.009017</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>0.014394</td>\n",
       "      <td>0.019023</td>\n",
       "      <td>0.044380</td>\n",
       "      <td>0.010251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>0.054114</td>\n",
       "      <td>0.039444</td>\n",
       "      <td>0.040532</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>0.022844</td>\n",
       "      <td>0.035089</td>\n",
       "      <td>0.013289</td>\n",
       "      <td>0.044411</td>\n",
       "      <td>0.030472</td>\n",
       "      <td>0.063550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054750</td>\n",
       "      <td>0.039671</td>\n",
       "      <td>0.031042</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.017207</td>\n",
       "      <td>0.044139</td>\n",
       "      <td>0.041389</td>\n",
       "      <td>0.047840</td>\n",
       "      <td>0.037428</td>\n",
       "      <td>0.056659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>0.038297</td>\n",
       "      <td>0.041555</td>\n",
       "      <td>0.028643</td>\n",
       "      <td>0.029425</td>\n",
       "      <td>0.025515</td>\n",
       "      <td>0.017616</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>0.050811</td>\n",
       "      <td>0.026241</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024817</td>\n",
       "      <td>0.040213</td>\n",
       "      <td>0.036483</td>\n",
       "      <td>0.017108</td>\n",
       "      <td>0.026987</td>\n",
       "      <td>0.032075</td>\n",
       "      <td>0.018335</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>0.017959</td>\n",
       "      <td>0.019455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>678 rows × 410 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            accruals_p1  accruals_p2  accruals_p3  accruals_p4  accruals_p5  \\\n",
       "date                                                                          \n",
       "1963-07-01    -0.012826    -0.035516    -0.001749    -0.002739     0.012370   \n",
       "1963-08-01     0.067188     0.075105     0.065833     0.059440     0.050219   \n",
       "1963-09-01     0.027254    -0.036061    -0.025227    -0.021122    -0.031047   \n",
       "1963-10-01     0.000835     0.018604     0.051991     0.028729     0.022873   \n",
       "1963-11-01    -0.007417     0.006792    -0.006046    -0.013141     0.000565   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2019-08-01    -0.011983    -0.026596    -0.033196    -0.019816    -0.017636   \n",
       "2019-09-01     0.011086     0.027976     0.009426     0.025268     0.021759   \n",
       "2019-10-01     0.041932     0.034752     0.040694    -0.004829     0.014374   \n",
       "2019-11-01     0.054114     0.039444     0.040532     0.025295     0.022844   \n",
       "2019-12-01     0.038297     0.041555     0.028643     0.029425     0.025515   \n",
       "\n",
       "            accruals_p6  accruals_p7  accruals_p8  accruals_p9  accruals_p10  \\\n",
       "date                                                                           \n",
       "1963-07-01     0.004111     0.003346    -0.005244    -0.005873      0.007966   \n",
       "1963-08-01     0.043807     0.052063     0.097553     0.046265      0.069205   \n",
       "1963-09-01    -0.020266     0.006704    -0.001180     0.000810     -0.001218   \n",
       "1963-10-01    -0.002550     0.033786     0.020920     0.068880      0.138922   \n",
       "1963-11-01    -0.012780     0.020039    -0.004045    -0.008319     -0.065160   \n",
       "...                 ...          ...          ...          ...           ...   \n",
       "2019-08-01    -0.034318     0.004716    -0.013897    -0.017711     -0.003340   \n",
       "2019-09-01     0.024613     0.017311     0.012674    -0.000337     -0.010182   \n",
       "2019-10-01     0.004451     0.002337     0.044002     0.005615      0.017040   \n",
       "2019-11-01     0.035089     0.013289     0.044411     0.030472      0.063550   \n",
       "2019-12-01     0.017616     0.019704     0.050811     0.026241      0.010913   \n",
       "\n",
       "            ...     sp_p1     sp_p2     sp_p3     sp_p4     sp_p5     sp_p6  \\\n",
       "date        ...                                                               \n",
       "1963-07-01  ...  0.001246  0.016596  0.003354  0.004370 -0.002039 -0.021691   \n",
       "1963-08-01  ...  0.045485  0.041143  0.048661  0.059708  0.069453  0.071549   \n",
       "1963-09-01  ... -0.002031 -0.029466 -0.023707 -0.004787 -0.016817 -0.015236   \n",
       "1963-10-01  ...  0.033032 -0.009433  0.030700  0.060448  0.021997  0.033764   \n",
       "1963-11-01  ...  0.018028 -0.012895 -0.011324 -0.029286 -0.011138 -0.007688   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "2019-08-01  ... -0.007943 -0.030568 -0.024177 -0.003217 -0.014945 -0.048403   \n",
       "2019-09-01  ... -0.019202  0.022650  0.037942  0.031047  0.036637  0.016883   \n",
       "2019-10-01  ...  0.028577  0.031458  0.015161  0.005760  0.009017  0.015486   \n",
       "2019-11-01  ...  0.054750  0.039671  0.031042  0.016766  0.017207  0.044139   \n",
       "2019-12-01  ...  0.024817  0.040213  0.036483  0.017108  0.026987  0.032075   \n",
       "\n",
       "               sp_p7     sp_p8     sp_p9    sp_p10  \n",
       "date                                                \n",
       "1963-07-01 -0.013344 -0.018734 -0.020539 -0.006043  \n",
       "1963-08-01  0.057740  0.047063  0.083413  0.043130  \n",
       "1963-09-01 -0.005143 -0.021049 -0.009069 -0.033984  \n",
       "1963-10-01  0.015734  0.017601  0.038851  0.009602  \n",
       "1963-11-01 -0.000038  0.011123 -0.004890 -0.003218  \n",
       "...              ...       ...       ...       ...  \n",
       "2019-08-01 -0.068869 -0.035887 -0.049141 -0.063963  \n",
       "2019-09-01  0.035709  0.039095  0.055311  0.060977  \n",
       "2019-10-01  0.014394  0.019023  0.044380  0.010251  \n",
       "2019-11-01  0.041389  0.047840  0.037428  0.056659  \n",
       "2019-12-01  0.018335  0.009325  0.017959  0.019455  \n",
       "\n",
       "[678 rows x 410 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Grab names of anomalous-return files\n",
    "PATH = 'DataSet/monthly/'\n",
    "FILE_PREFIX = 'ret10_'\n",
    "FILE_EXTENSION = '.csv'\n",
    "filenames = [filename for filename in os.listdir(PATH) if filename.startswith(FILE_PREFIX)]\n",
    "print(f'Found {len(filenames)} files of anomalous-return data')\n",
    "# Load files one by one into same dataframe\n",
    "merged = pd.DataFrame(index=pd.DatetimeIndex([], name='date'))\n",
    "non_decile, missing_data, complete = (set(), set(), set()) # keep track of data completeness while loading\n",
    "for filename in filenames:\n",
    "    characteristic_of = lambda fn: fn[len(FILE_PREFIX):-len(FILE_EXTENSION)]\n",
    "    df = pd.read_csv(PATH + filename, index_col='date', parse_dates=True, date_format='%m/%Y')\n",
    "    ### Preliminary cleaning step: skip nondecile characteristics / other missing data\n",
    "    def is_missing_data(df):\n",
    "        # Non-decile characteristics have columns populated entirely with NaN\n",
    "        for col in df.columns:\n",
    "            if df[col].count() == 0:\n",
    "                non_decile.add(characteristic_of(filename))\n",
    "                return True\n",
    "        # Scan for missing cells in populated columns as well\n",
    "        for col in df.columns:\n",
    "            if df[col].hasnans:\n",
    "                missing_data.add(characteristic_of(filename))\n",
    "                return True\n",
    "        complete.add(characteristic_of(filename))\n",
    "        return False\n",
    "    if is_missing_data(df):\n",
    "        continue\n",
    "    ### End preliminary cleaning step\n",
    "    df.rename(columns=lambda c: f'{characteristic_of(filename)}_{c}', inplace=True) # prefix columns with characteristic\n",
    "    merged = merged.merge(df, 'outer', 'date')\n",
    "    # Outer join to keep months that are missing whole characteristics for now\n",
    "    # These will be handled in a later cleaning step\n",
    "print('Summary:')\n",
    "print(f'\\t{len(non_decile)} non-decile characteristics: {non_decile}')\n",
    "print(f'\\t{len(missing_data)} characteristics with missing cells: {missing_data}')\n",
    "print(f'\\t{len(complete)} characteristics with complete data: {complete}')\n",
    "print('Using only characteristics with complete data')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae79895-1747-4132-b3cd-abef9370014b",
   "metadata": {},
   "source": [
    "#### Cross-reference with PCA paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef1615-102f-47e9-9fa8-0da59c8b41c3",
   "metadata": {},
   "source": [
    "Since we're comparing the performance of the sequential autoencoder to the performance of economics-informed PCA from Bryzgalova et al, it's worth seeing how our set of characteristics compares to the 37 used in that paper. (Eliminating characteristics with incomplete data gets us close but not all the way to 37.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497fbca0-c401-4ef9-8214-735ae3257e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characteristics that are in the 37 but NOT complete: set()\n",
      "Characteristics that are complete but NOT in the 37: {'dur', 'divg', 'betaarb', 'shortint'}\n"
     ]
    }
   ],
   "source": [
    "# Check existence & completeness of the 37 in our data set\n",
    "firm_specific_characteristics = {\n",
    "    'reversal': {'lrrev', 'strev', 'indmomrev', 'indrrev', 'indrrevlv'},\n",
    "    'value interaction': {'valmom', 'valmomprof', 'valprof'},\n",
    "    'investment': {'inv', 'invcap', 'igrowth', 'growth', 'noa'},\n",
    "    'momentum': {'mom', 'mom12', 'indmom', 'momrev'},\n",
    "    'value': {'value', 'valuem', 'divp', 'ep', 'cfp', 'sp'},\n",
    "    'other': {'size', 'price', 'accruals', 'ciss', 'gmargins', 'lev', 'season', 'sgrowth'},\n",
    "    'trading frictions': {'ivol', 'shvol', 'aturnover'},\n",
    "    'profitability': {'prof', 'roaa', 'roea'}\n",
    "}\n",
    "\n",
    "characteristic_set = set()\n",
    "for chars in firm_specific_characteristics.values():\n",
    "    characteristic_set = characteristic_set.union(chars)\n",
    "if len(characteristic_set) != 37:\n",
    "    raise ValueError(len(characteristic_set)) # sanity check\n",
    "# for characteristic in characteristic_set:\n",
    "#     if characteristic in non_decile:\n",
    "#         print(f'{characteristic}: non_decile')\n",
    "#     elif characteristic in missing_data:\n",
    "#         print(f'{characteristic}: missing')\n",
    "#     elif characteristic in complete:\n",
    "#         print(f'{characteristic}: complete')\n",
    "#     else:\n",
    "#         print(f'{characteristic}: ???')\n",
    "print(f'Characteristics that are in the 37 but NOT complete: {characteristic_set.difference(complete)}')\n",
    "print(f'Characteristics that are complete but NOT in the 37: {complete.difference(characteristic_set)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ded8c3-998e-4327-b069-60629ccfe67b",
   "metadata": {},
   "source": [
    "Our data set is complete with respect to these 37 characteristics, and has four additional characteristics that may or may not be useful. These can be ignored for the purposes of cross-referencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61bea8c-38e4-4b52-9686-0406ae02371a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c6feb\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c6feb_level0_col0\" class=\"col_heading level0 col0\" >Category</th>\n",
       "      <th id=\"T_c6feb_level0_col1\" class=\"col_heading level0 col1\" >Bottom-Decile Mean</th>\n",
       "      <th id=\"T_c6feb_level0_col2\" class=\"col_heading level0 col2\" >Top-Decile Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c6feb_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c6feb_row0_col0\" class=\"data row0 col0\" >reversal</td>\n",
       "      <td id=\"T_c6feb_row0_col1\" class=\"data row0 col1\" >0.58%</td>\n",
       "      <td id=\"T_c6feb_row0_col2\" class=\"data row0 col2\" >1.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6feb_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c6feb_row1_col0\" class=\"data row1 col0\" >value interaction</td>\n",
       "      <td id=\"T_c6feb_row1_col1\" class=\"data row1 col1\" >0.72%</td>\n",
       "      <td id=\"T_c6feb_row1_col2\" class=\"data row1 col2\" >1.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6feb_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c6feb_row2_col0\" class=\"data row2 col0\" >investment</td>\n",
       "      <td id=\"T_c6feb_row2_col1\" class=\"data row2 col1\" >0.79%</td>\n",
       "      <td id=\"T_c6feb_row2_col2\" class=\"data row2 col2\" >1.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6feb_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c6feb_row3_col0\" class=\"data row3 col0\" >momentum</td>\n",
       "      <td id=\"T_c6feb_row3_col1\" class=\"data row3 col1\" >0.69%</td>\n",
       "      <td id=\"T_c6feb_row3_col2\" class=\"data row3 col2\" >1.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6feb_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c6feb_row4_col0\" class=\"data row4 col0\" >value</td>\n",
       "      <td id=\"T_c6feb_row4_col1\" class=\"data row4 col1\" >0.82%</td>\n",
       "      <td id=\"T_c6feb_row4_col2\" class=\"data row4 col2\" >1.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6feb_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c6feb_row5_col0\" class=\"data row5 col0\" >other</td>\n",
       "      <td id=\"T_c6feb_row5_col1\" class=\"data row5 col1\" >0.81%</td>\n",
       "      <td id=\"T_c6feb_row5_col2\" class=\"data row5 col2\" >1.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6feb_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c6feb_row6_col0\" class=\"data row6 col0\" >trading frictions</td>\n",
       "      <td id=\"T_c6feb_row6_col1\" class=\"data row6 col1\" >0.64%</td>\n",
       "      <td id=\"T_c6feb_row6_col2\" class=\"data row6 col2\" >0.96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c6feb_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c6feb_row7_col0\" class=\"data row7 col0\" >profitability</td>\n",
       "      <td id=\"T_c6feb_row7_col1\" class=\"data row7 col1\" >0.81%</td>\n",
       "      <td id=\"T_c6feb_row7_col2\" class=\"data row7 col2\" >1.03%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fefff5ad510>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-reference summary statistics\n",
    "# TODO Sharpe ratio?\n",
    "categories = []\n",
    "top_decile_means = []\n",
    "bottom_decile_means = []\n",
    "for category, characteristics in firm_specific_characteristics.items():\n",
    "    top_decile_sum = 0\n",
    "    bottom_decile_sum = 0\n",
    "    count = 0\n",
    "    for characteristic in characteristics:\n",
    "        top_decile_sum += merged[characteristic + '_p10'].iloc[4:-24].sum()\n",
    "        bottom_decile_sum += merged[characteristic + '_p1'].iloc[4:-24].sum()\n",
    "        count += merged[characteristic + '_p1'].iloc[4:-24].count() # same for all columns but this is an easy way to get at it\n",
    "    categories.append(category)\n",
    "    top_decile_means.append(top_decile_sum / count)\n",
    "    bottom_decile_means.append(bottom_decile_sum / count)\n",
    "summary_stats = pd.DataFrame()\n",
    "summary_stats['Category'] = categories\n",
    "summary_stats['Bottom-Decile Mean'] = bottom_decile_means\n",
    "summary_stats['Top-Decile Mean'] = top_decile_means\n",
    "summary_stats.style.format({'Bottom-Decile Mean': '{:.2%}'.format, 'Top-Decile Mean': '{:.2%}'.format})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2478d3d6-36a2-4080-91c6-7e976d206ce6",
   "metadata": {},
   "source": [
    "#### Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b5bc7-6bf0-4cf0-b058-8b362f335723",
   "metadata": {},
   "source": [
    "TODO check for malformed data, data-entry errors resulting in outliers, etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddef46c-6fdc-4d26-9389-7074e71fec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = merged.loc[:,[col[:col.index('_')] in characteristic_set for col in merged.columns]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e0f94-8e53-4c99-8d29-2205e60acfae",
   "metadata": {},
   "source": [
    "Another kind of missing data: since characteristics don't all cover the same date range, some months are missing whole characteristics. These show up as NaNs - see if restricting to the same date range in the paper will clean them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d39440bc-17a2-4468-afc8-b09dad5f4158",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleaned.iloc[4:-24]\n",
    "for column in cleaned:\n",
    "    if cleaned[column].hasnans:\n",
    "        raise ValueError(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ef8d1-62cc-4e4f-a6c1-ec9c2b8d6020",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c87d9-f4d8-4116-9b64-61639e7417c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Taking the merged data frame and restricting it to only features from the paper would give us 370 columns. However, it appears the original paper may be doing PCA on 10 subsets of those 370 columns broken out by decile: all p1 columns, all p2 columns, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91b3c93-fb7c-42ce-ab4a-3fb65a96efd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.831081973491824\n",
      "0.864993854319867\n",
      "0.8915781703558645\n",
      "0.9115900416780917\n",
      "0.9239539970181607\n",
      "0.9327620274685808\n",
      "0.9400295606692647\n",
      "0.9455909091338196\n",
      "0.9506332609039029\n",
      "0.955164447805204\n",
      "0.9595258148223405\n",
      "0.962915342023093\n",
      "0.9661541644866124\n",
      "0.9693118369519836\n",
      "0.9721298587592181\n",
      "0.9746866371688956\n",
      "0.9771311903315486\n",
      "0.9792922150102983\n",
      "0.981270695727983\n",
      "0.9831998503593042\n",
      "0.9850482713858716\n",
      "0.986729073001825\n",
      "0.9883359569413405\n",
      "0.9898634947567821\n",
      "0.9912660552869582\n",
      "0.9924175654537427\n",
      "0.9934837224372723\n",
      "0.9945234366908379\n",
      "0.9954705168420339\n",
      "0.9962901645250891\n",
      "0.9970823630340926\n",
      "0.99781315141166\n",
      "0.9984622235313597\n",
      "0.9990695850748799\n",
      "0.9995801914754863\n",
      "0.9998393681431762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "def pca_plot(df):\n",
    "    scaled = StandardScaler().fit_transform(df)\n",
    "    for i in range(0, len(df.columns)):\n",
    "        pca = PCA(i)\n",
    "        pca.fit(scaled)\n",
    "        print(sum(pca.explained_variance_ratio_))\n",
    "        # TODO plot it instead\n",
    "# Take one of the deciles\n",
    "p1_subframe = cleaned.loc[:,['1' == col[col.index('_')+2:] for col in cleaned.columns]]\n",
    "# TODO build rest of subframes\n",
    "pca_plot(p1_subframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaff9cc-bbd2-4559-a36c-f09397328fbd",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c596150-ae8d-4137-b533-6eba09dfb885",
   "metadata": {},
   "source": [
    "## Thoughts & questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464c4a6-285e-4ffe-91cd-7e265d422548",
   "metadata": {},
   "source": [
    "- Is this the right data set from the paper? That one says 37 characteristics, this data set has 55\n",
    "  - Also their sample period is shorter (11/1963 to 12/2017 - we have 07/1963 to 12/2019)\n",
    "  - Take the 37 & throw out the rest? (See if cleaning up the non-decile ones does that anyway) -> nope, reduces to 50\n",
    "  - What about also removing the ones with empty cells? -> nope, 41\n",
    "    - Reasons why the remaining 4 complete ones aren't relevant isn't apparent from their descriptions\n",
    "  - Summary stats = Table B.1? Top & bottom decile means + Sharpe ratio (top - bottom)?\n",
    "    \n",
    "- Confirm that data format is:\n",
    "  - Rows: months from the time series\n",
    "  - Columns: all combinations of (characteristic, decile)\n",
    "  - Effect is like concatting the 55 tables sideways to make 550 columns (and naming each tablename+columnname for uniqueness)\n",
    "  - Not inherently sparse - each combination of characteristic & decile does have a cell in the original data\n",
    "  e.g.\n",
    "\n",
    "  | Date       | 1st-decile price | 2nd-decile price | ... | 1st-decile market cap | 2nd-decile market cap | ... |\n",
    "  | ---        | ---              | ---              | --- | ---                   | ---                   | --- |\n",
    "  | Sept. 1971 | return cell 1A   | return cell 1B   | ... | etc.                  | etc.                  | ... |\n",
    "  | Oct. 1971  | return cell 2A   | return cell 2B   | ... | etc.                  | etc.                  | ... |\n",
    "  | ...        | ...              | ...              | ... | ...                   | ...                   | ... |\n",
    "  \n",
    "  \n",
    "- How do the factors apply to an individual stock which occupies only one decile for each characteristic?\n",
    "  - For prediction, presumably its return from period t would go into the autoencoder as a sparse vector populated by characteristics, i.e. if its price is in the 6th decile then its return goes into the 6th-decile price input and all other price inputs get 0 - repeat for its other characteristics.\n",
    "  - This answers the question of why it's not a problem that stocks move between deciles as time goes by. If we're using returns at t to predict returns at t+1 then we only care about the stock's characteristics at t\n",
    "  - OK but how do the factors correspond to a linear combination of stocks? Is \"a stock\" not bound to a company i.e. GOOG or CSX but rather to a set of parameters like (price_decile=A, marketcap_decile=B, etc.) such that any company can assume the role of that stock when its shares match those parameters?\n",
    "    \n",
    "- Deciles are ordinal data and here we are treating them like categorical. Isn't that throwing out perfectly good data? I assume (and Bryzgalova et al back me up with what they say about monotonicity) that if we know how it affects returns to have a price in the first decile, then the effect of a price in the second decile should be closely related. Will the factors just pick that up on their own? (Or maybe we do something convolution-like between input & compression layer?)\n",
    "\n",
    "- Data cleaning TODO above\n",
    "\n",
    "- What if broken-stick method gives a different optimal number of factors for different deciles' subsets of the data?\n",
    "\n",
    "- Use straight PCA results to estimate how much extra variance the economic information in the paper's model can explain?\n",
    "\n",
    "- The exploration/cleaning of the data set is kind of a mess: it goes back and forth between trying to find the biggest clean data set possible from first principles and trying to use the exact subset of rows/columns from the paper (verifying that this subset is also clean). How much of the mess needs to be kept as a show of reasoning process and how much can be refactored? (Git history = process?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba8c66-fa8f-4ad6-b862-a893f32df5f9",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8776c9a-d556-45a9-9757-5cf3871bec2c",
   "metadata": {},
   "source": [
    "#### Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc47bda2-6acc-4c60-a786-42fd2e9d0e3c",
   "metadata": {},
   "source": [
    "1. Idea and ongoing guidance provided by Cameron Fen\n",
    "1. Haddad, Kozak, Santosh (2020) \"Factor Timing\"\n",
    "1. Giglio, Kelly, Kozak (2020) \"Equity Term Structures without Dividend Strips Data\"\n",
    "1. [Data set provided by Serhiy Kozak](https://drive.google.com/file/d/1ocP9FPtZQtfYetqvsPkUwwqvbAcyU1qe/view?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
